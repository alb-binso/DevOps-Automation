### DynamoDB to PostgreSQL ETL
### Overview

This project is a Python-based ETL (Extract, Transform, Load) script that migrates data from AWS DynamoDB tables into a PostgreSQL database.

It is designed for the Tymeplus / Foresyts HR domain, handling multiple master and transactional tables while preserving referential integrity, handling pagination, and performing upserts (insert or update on conflict).

### Features

‚úÖ Secure configuration using .env

‚úÖ Automatic PostgreSQL table creation

‚úÖ DynamoDB pagination handling

‚úÖ Decimal to native type conversion

‚úÖ Foreign key dependency handling

‚úÖ Idempotent migrations using ON CONFLICT

‚úÖ Skips invalid or inactive employee records safely

‚úÖ Detailed console logs for tracking progress

Data Flow (High Level)
DynamoDB Tables
   ‚Üì
Data Cleaning & Type Conversion
   ‚Üì
PostgreSQL Tables (Upsert)

DynamoDB Tables Used
DynamoDB Table	Purpose
tymeplusDepartmentMaster	Department master data
tymeplusLeaveCategories	Leave type master
tymeplusHolidayMaster	Holiday calendar
tymeplusUserAuth	Employee master
tymeplusUserLeaves	Leave transactions
tymeplusUserAbsentList	Absent records
PostgreSQL Tables Created
PostgreSQL Table	Description
foresytsdepartment	Department master
foresytleavetype	Leave category master
foresytsholidays	Holidays
foresytsemployee_master	Employee master
foresytsleavelist	Leave records
foresytsabsentlist	Absent records

Tables are created automatically if they do not exist.

Prerequisites

Python 3.8+

AWS credentials with DynamoDB read access

PostgreSQL 12+

Network access to both DynamoDB and PostgreSQL

Python Dependencies

Install required packages:

pip install boto3 psycopg2-binary python-dotenv

Environment Configuration

Create a file named foremain.env in the project root.

# AWS Configuration
AWS_REGION=ap-south-1
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key

# PostgreSQL Configuration
PG_HOST=localhost
PG_PORT=5432
PG_DATABASE=your_db
PG_USER=your_user
PG_PASSWORD=your_password

# Client Configuration
CLIENT_ID=WASJKSP
üìä ETL Architecture Diagram
1Ô∏è‚É£ High-Level Data Flow (ASCII)
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ   foremain.env        ‚îÇ
                ‚îÇ (AWS + PG configs)   ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        DynamoDBToPostgresETL (Python)             ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  - boto3 (DynamoDB)                               ‚îÇ
‚îÇ  - psycopg2 (PostgreSQL)                          ‚îÇ
‚îÇ  - dotenv                                         ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  - Decimal ‚Üí native type conversion               ‚îÇ
‚îÇ  - Date & timestamp parsing                       ‚îÇ
‚îÇ  - Pagination handling                            ‚îÇ
‚îÇ  - Upserts with ON CONFLICT                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                AWS DynamoDB                      ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  tymeplusDepartmentMaster                        ‚îÇ
‚îÇ  tymeplusLeaveCategories                         ‚îÇ
‚îÇ  tymeplusHolidayMaster                           ‚îÇ
‚îÇ  tymeplusUserAuth                                ‚îÇ
‚îÇ  tymeplusUserLeaves                              ‚îÇ
‚îÇ  tymeplusUserAbsentList                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                PostgreSQL                         ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  foresytsdepartment                               ‚îÇ
‚îÇ  foresytleavetype                                 ‚îÇ
‚îÇ  foresytsholidays                                 ‚îÇ
‚îÇ  foresytsemployee_master                          ‚îÇ
‚îÇ  foresytsleavelist                                ‚îÇ
‚îÇ  foresytsabsentlist                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

2Ô∏è‚É£ Migration Dependency Flow

This shows why the migration order matters:

Departments
     ‚îÇ
     ‚ñº
Employees ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                   ‚îÇ
     ‚ñº                   ‚ñº
Leave Types           Holidays
     ‚îÇ                   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚ñº
            Leaves
               ‚îÇ
               ‚ñº
            Absents


‚úî Foreign keys are respected
‚úî Invalid employees are skipped safely

3Ô∏è‚É£ GitHub-Rendered Diagram (Mermaid)

‚úÖ GitHub automatically renders this
‚ùå Do NOT use on platforms without Mermaid support

flowchart TD
    ENV[foremain.env] --> ETL[DynamoDBToPostgresETL]

    ETL --> D1[tymeplusDepartmentMaster]
    ETL --> D2[tymeplusLeaveCategories]
    ETL --> D3[tymeplusHolidayMaster]
    ETL --> D4[tymeplusUserAuth]
    ETL --> D5[tymeplusUserLeaves]
    ETL --> D6[tymeplusUserAbsentList]

    D1 --> P1[foresytsdepartment]
    D2 --> P2[foresytleavetype]
    D3 --> P3[foresytsholidays]
    D4 --> P4[foresytsemployee_master]
    D5 --> P5[foresytsleavelist]
    D6 --> P6[foresytsabsentlist]

4Ô∏è‚É£ ETL Execution Sequence Diagram
Start
  ‚îÇ
  ‚ñº
Load Environment Variables
  ‚îÇ
  ‚ñº
Connect to PostgreSQL
  ‚îÇ
  ‚ñº
Create Tables (if not exists)
  ‚îÇ
  ‚ñº
Migrate Departments
  ‚îÇ
  ‚ñº
Migrate Leave Types
  ‚îÇ
  ‚ñº
Migrate Holidays
  ‚îÇ
  ‚ñº
Migrate Employees
  ‚îÇ
  ‚ñº
Migrate Leaves (validate employee)
  ‚îÇ
  ‚ñº
Migrate Absents (validate employee)
  ‚îÇ
  ‚ñº
Close Connections
  ‚îÇ
  ‚ñº
End

How to Run
python etl.py


Replace etl.py with the actual filename if different.

Execution Order (Important)

The script migrates data in dependency-safe order:

Departments

Leave Types

Holidays

Employees

Leaves

Absents

This prevents foreign key violations.

Key Behaviors
Decimal Handling

DynamoDB Decimal values are automatically converted to int or float.

Date & Timestamp Parsing

Supports:

ISO timestamps (2024-01-01T10:00:00Z)

Date-only strings (YYYY-MM-DD)

Invalid or empty values are safely converted to NULL.

Employee Validation

Leave and absent records are skipped if the employee does not exist or is inactive.

Skipped records are logged clearly.

Error Handling

Any failure:

Rolls back PostgreSQL transactions

Prints a clear error message

Database connections are always closed safely.

Sample Console Output
============================================================
### DynamoDB to PostgreSQL ETL Process
============================================================
‚úì Connected to PostgreSQL
‚úì Tables created/verified

‚Üí Migrating departments...
‚úì Migrated 12 departments

‚Üí Migrating employees...
‚úì Migrated 245 employees

‚Üí Migrating leaves...
‚ö† Skipping leave record for non-existent/inactive employee: U123
‚úì Migrated 1,420 leave records

### ‚úì ETL Process Completed Successfully!
============================================================
‚úì Database connections closed

Customization

You can easily:

Change CLIENT_ID to migrate a different tenant

Disable specific migrations by commenting methods in run()

Schedule execution via cron / Airflow / Jenkins

### Notes & Best Practices

Recommended to run during low traffic hours

Ensure DynamoDB scan limits are acceptable for large datasets

Consider adding CloudWatch logging for production usage

### Author
Leo
Backend / Data Engineer
ETL ‚Ä¢ AWS ‚Ä¢ PostgreSQL ‚Ä¢ Python